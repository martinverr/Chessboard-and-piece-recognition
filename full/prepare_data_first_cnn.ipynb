{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script prepare data square occupation\n",
    "Script to create training image data for the CNN that classify an empty or occupied square.\n",
    "\n",
    "For each single square create:\n",
    "- Image named as \"<dataset image no.>_<square coord>.png\"\n",
    "- Text file \"<dataset image no.>_<square coord>.txt\" containing the true label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob\n",
    "import cv2\n",
    "from FEN import FEN\n",
    "from chessboard_detection import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change path according to where you have the dataset folder, default input dir\n",
    "\n",
    "Change path according to where you prefer having the output\n",
    "\n",
    "Regex:\n",
    "- path/1** : start with '1' (1000 to 1999)\n",
    "- path/** : all\n",
    "\n",
    "rewrite: Set 'True' to rewrite old already processed images found in the output dir "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img_path = './../input/**'\n",
    "dst_dir_path = './../output/training_squares/'\n",
    "rewrite = True\n",
    "rewrite_errors = True\n",
    "dataset_percentage = 1\n",
    "save_output = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "with open(\"errors.txt\", \"r\") as file:\n",
    "  error_list = ast.literal_eval(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if rewrite_errors:\n",
    "    error_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(dst_dir_path, exist_ok=True)\n",
    "\n",
    "to_be_processed = glob.glob(input_img_path)\n",
    "if not rewrite:\n",
    "    already_processed = glob.glob(f'{dst_dir_path}**')\n",
    "    already_processed = list(set([os.path.splitext(filename)[0].split('\\\\')[-1].split('_')[0] for filename in already_processed]))\n",
    "\n",
    "already_processed = []\n",
    "already_processed.sort()\n",
    "\n",
    "all_input_files = glob.glob(input_img_path)\n",
    "to_be_processed = [filename for filename in all_input_files\n",
    "                   if '.json' not in filename\n",
    "                   and os.path.splitext(filename)[0].split('\\\\')[-1] not in already_processed\n",
    "                   and os.path.splitext(filename)[0].split('\\\\')[-1] not in error_list\n",
    "                   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cut_index = int(len(to_be_processed) * dataset_percentage - len(already_processed))\n",
    "to_be_processed = to_be_processed[:cut_index]\n",
    "to_be_processed.sort()\n",
    "\n",
    "if cut_index < 0:\n",
    "  exceeded = True\n",
    "\n",
    "print(f\"Already processed: {len(already_processed)}\")\n",
    "print(f\"To be processed: {len(to_be_processed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MaskRCNN_board() \n",
    "model.model.load_state_dict(torch.load('./../maskRCNN_epoch_4_.pth', map_location = device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_to_be_processed = len(to_be_processed)\n",
    "num_of_already_processed = 0\n",
    "last_progress_percentage_shown = 0\n",
    "\n",
    "for in_process in to_be_processed:\n",
    "    imgnumber = os.path.splitext(in_process)[0].split('\\\\')[-1]\n",
    "\n",
    "    # percentage update\n",
    "    progress_percentage = num_of_already_processed / num_of_to_be_processed * 100\n",
    "    if progress_percentage - last_progress_percentage_shown > 10:\n",
    "        last_progress_percentage_shown = progress_percentage\n",
    "        print(f\"########################### progress: {progress_percentage}% ###########################\\n\")\n",
    "\n",
    "    #skip if not a file image, if json does not exists, if already present in output\n",
    "    if not os.path.isfile(in_process):\n",
    "        continue\n",
    "    if not in_process.lower().endswith(\".png\"):\n",
    "        continue\n",
    "    print(f\"{in_process}...\", end=' ')\n",
    "    if not os.path.isfile(os.path.splitext(in_process)[0] + '.json'):\n",
    "        print(f\"Not found related json({in_process})\")\n",
    "        continue\n",
    "    if imgnumber in already_processed:\n",
    "        num_of_already_processed += 1\n",
    "        print(f\"Already processed, skipped({in_process})\")\n",
    "        continue\n",
    "\n",
    "    # load FEN true label\n",
    "    truth = FEN(os.path.splitext(in_process)[0])\n",
    "    true_fen, true_pos, viewpoint = truth.fen, truth.pieces, truth.view\n",
    "    \n",
    "    try:\n",
    "        # First pass preprocessing\n",
    "        warpedBoardImg = board_detection(in_process, old_version=False, model=model)\n",
    "        if warpedBoardImg is None:\n",
    "            num_of_already_processed += 1\n",
    "            print(\"Skipped (Error in warping)\")\n",
    "            raise Exception(\"Warping Error\")\n",
    "\n",
    "        # Second pass preprocessing\n",
    "        grid_squares = grid_detection(warpedBoardImg, viewpoint)\n",
    "        if grid_squares is None:\n",
    "            print(\"Skipped (Error in grid)\")\n",
    "            num_of_already_processed += 1\n",
    "            raise Exception(\"Grid Error\")\n",
    "        \n",
    "        # Extend the information to include piece information in 3rd col (image remain last in 4th col)\n",
    "        grid_squares = np.column_stack((grid_squares[:,:2], \n",
    "                                            [true_pos.get(coord, 'empty') for coord in grid_squares[:, 1]],\n",
    "                                            grid_squares[:,-2:]\n",
    "                                            ))\n",
    "        \n",
    "        for square_coord, piece, square_img in grid_squares[:,1:4]:\n",
    "            output_filename = f'{dst_dir_path}{imgnumber}_{square_coord}'\n",
    "            \n",
    "            # .png\n",
    "            cv2.imwrite(output_filename + '.png', square_img)\n",
    "\n",
    "            # .txt\n",
    "            with open(f'{output_filename}.txt', 'w') as f:\n",
    "                f.write(true_pos[square_coord] if square_coord in true_pos else 'empty')\n",
    "\n",
    "        print('Done')\n",
    "\n",
    "    except:\n",
    "        output_filename = f'{dst_dir_path}{imgnumber}_error'\n",
    "        with open(f'{output_filename}.txt', 'w') as f:\n",
    "                f.write('Error somewhere')\n",
    "            \n",
    "    num_of_already_processed += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean square files created by image that raised errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = glob.glob(f'{dst_dir_path}**error**')\n",
    "\n",
    "errors = list(set([os.path.splitext(filename)[0].split('\\\\')[-1].split('_')[0] for filename in errors]))\n",
    "print(f\"Found {len(errors)} to be deleted\")\n",
    "\n",
    "for error in errors:\n",
    "    for filename in glob.glob(f\"{dst_dir_path}*{error}*\"):\n",
    "        os.remove(filename)\n",
    "\n",
    "errors.sort()\n",
    "print(f\"errors deleted: {errors}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "already_processed = glob.glob(f'{dst_dir_path}**')\n",
    "already_processed = list(set([os.path.splitext(filename)[0].split('\\\\')[-1].split('_')[0] for filename in already_processed]))\n",
    "\n",
    "total_errors = [f'{i:0>4}' for i in range(len(all_input_files)//2) if f'{i:0>4}' not in already_processed]\n",
    "total_errors.sort()\n",
    "\n",
    "with open(\"errors.txt\", \"w\") as f:\n",
    "  print(f\"In total {len(total_errors)} errors, save in errors.txt\")\n",
    "  f.write(str(total_errors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Update img numbers that of imgs that raised an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"errors.txt\", \"w\") as f:\n",
    "  total_errors = list(set(error_list + errors))\n",
    "  print(f\"In total {len(total_errors)} errors, save in errors.txt\")\n",
    "  f.write(str(list(set(error_list + errors))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
